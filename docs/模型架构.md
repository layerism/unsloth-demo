### **第一步：如何嵌入**

输入：`i am xiaoming，i love xiaohong`

Tokenizer：`i am xiao ming，i lo ve xiao hon g`
Token-id：`12 34 45 57 89 123 456 789 345 678 578`
Embedding: `vocab_size x 4096`
Emb-table-lookup: `11 x 4096`

问题：`xiaohong’s boy friend` 

类比图灵机：

1. 回查query，get，for，移位，break → `xiao ming`
2. `boy friend` → `love` → `am`
3. `xiao ming` →  `am`

大模型电路逻辑，机械可解释性

### 第二步：**注意力计算**

输入 $x = [e_1, e_2, e_3, …, e_{11}]$

查询功能 query head：$q_i = W_q \cdot e_i$

被查询功能 key head: $k_i = W_k \cdot e_i$

查询结果 value head: $v_i = W_v \cdot e_i$

双向注意力：BERT model，diffusion 生成

因果注意力：GPT model，自回归生成

查询方式（往回看）：$q_i \cdot k_j$，比如说：$q_5 \to [k_1, k_2, k_3, k_4, k_5]$，假设 $q_5 \cdot k_3$ 最大，查询值 $v_3$

softmax-attention：$\sum_{j} \alpha_j v_j$，如果 $q\cdot k_j$ 很大，则  $\alpha_j$ 接近 1

$h = FFN(\sum_j\alpha_j v_j)$

### 第三步：可并行性

$q_5 \to [k_1, k_2, k_3, k_4, k_5]$

$q_6 \to [k_1, k_2, k_3, k_4, k_5,k_6]$

$q_7 \to [k_1, k_2, k_3, k_4, k_5,k_6,k_7]$

$q_8 \to [k_1, k_2, k_3, k_4, k_5,k_6,k_7,k_8]$

torch append

KV 缓存：$[k_1, k_2, k_3,k_4,k_5]$

$[v_1, v_2, v_3, v_4, v_5]$